#!/usr/bin/env python3
"""
Test Data Validator for Leaf Application

This script validates the test_data.csv file generated by generate_test_data.py
to ensure it meets all specified requirements.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import sys
import os
from typing import List, Tuple, Dict, Any
import base64

# Set UTF-8 encoding for Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

class TestDataValidator:
    def __init__(self, filename: str = 'test_data.csv'):
        self.filename = filename
        self.df = None
        self.errors = []
        self.warnings = []
        self.info = []
        
    def load_data(self) -> bool:
        """Load the CSV file."""
        try:
            # Load with keep_default_na=False to preserve empty strings
            self.df = pd.read_csv(self.filename, dtype=str, keep_default_na=False)
            self.info.append(f"‚úÖ Loaded {len(self.df)} rows from {self.filename}")
            return True
        except Exception as e:
            self.errors.append(f"‚ùå Failed to load {self.filename}: {e}")
            return False
    
    def validate_time_format(self, time_str: str, format_pattern: str) -> bool:
        """Validate time string matches expected format."""
        if format_pattern == "HH:MM:SS.sss":
            pattern = r'^\d{2}:\d{2}:\d{2}\.\d{3}$'
        elif format_pattern == "HH:MM:SS":
            pattern = r'^\d{2}:\d{2}:\d{2}$'
        elif format_pattern == "HH:MM:SS.ssssss":
            pattern = r'^\d{2}:\d{2}:\d{2}\.\d{6}$'
        elif format_pattern == "HH:MM:SS.sssssssss":
            pattern = r'^\d{2}:\d{2}:\d{2}\.\d{9}$'
        else:
            return False
        
        return bool(re.match(pattern, time_str))
    
    def parse_time(self, time_str: str) -> datetime:
        """Parse time string to datetime for comparison."""
        if '.' in time_str:
            # Handle milliseconds/microseconds/nanoseconds
            time_parts = time_str.split(':')
            hours = int(time_parts[0])
            minutes = int(time_parts[1])
            seconds_fraction = time_parts[2].split('.')
            seconds = int(seconds_fraction[0])
            
            # Convert fraction to microseconds (max precision datetime supports)
            fraction_str = seconds_fraction[1]
            if len(fraction_str) <= 6:
                microseconds = int(fraction_str.ljust(6, '0'))
            else:
                # Truncate nanoseconds to microseconds
                microseconds = int(fraction_str[:6])
            
            return datetime(2024, 1, 1, hours, minutes, seconds, microseconds)
        else:
            return datetime.strptime(f"2024-01-01 {time_str}", "%Y-%m-%d %H:%M:%S")
    
    def validate_good_time(self):
        """Validate good_time column."""
        print("\nüîç Validating good_time column...")
        
        # Check format
        invalid_format = []
        for idx, time_val in enumerate(self.df['good_time']):
            if not self.validate_time_format(time_val, "HH:MM:SS.sss"):
                invalid_format.append((idx, time_val))
        
        if invalid_format:
            self.errors.append(f"‚ùå Found {len(invalid_format)} rows with invalid good_time format")
            for idx, val in invalid_format[:5]:  # Show first 5
                self.errors.append(f"   Row {idx}: '{val}'")
        else:
            self.info.append("‚úÖ All good_time values have correct format")
        
        # Check strictly increasing (allowing for day wraparound)
        times = []
        for time_str in self.df['good_time']:
            try:
                times.append(self.parse_time(time_str))
            except:
                pass  # Already reported as format error
        
        non_increasing = []
        for i in range(1, len(times)):
            # Allow wraparound from 23:xx to 00:xx
            if times[i] < times[i-1]:
                # Check if this is a day wraparound
                time_diff = (times[i-1] - times[i]).total_seconds()
                if time_diff > 23 * 3600:  # More than 23 hours difference suggests wraparound
                    continue  # This is OK, it's a day wraparound
                non_increasing.append((i-1, i, self.df['good_time'].iloc[i-1], self.df['good_time'].iloc[i]))
        
        if non_increasing:
            self.errors.append(f"‚ùå Found {len(non_increasing)} instances where good_time goes backwards")
            for prev_idx, curr_idx, prev_time, curr_time in non_increasing[:5]:
                self.errors.append(f"   Rows {prev_idx}->{curr_idx}: {prev_time} -> {curr_time}")
        else:
            self.info.append("‚úÖ good_time is strictly increasing (with day wraparounds allowed)")
        
        # Check duplicates (1-5 times)
        duplicate_counts = []
        i = 0
        while i < len(self.df):
            count = 1
            current_time = self.df['good_time'].iloc[i]
            while i + count < len(self.df) and self.df['good_time'].iloc[i + count] == current_time:
                count += 1
            duplicate_counts.append(count)
            if count < 1 or count > 5:
                self.warnings.append(f"‚ö†Ô∏è  Row {i}: good_time duplicated {count} times (expected 1-5)")
            i += count
        
        self.info.append(f"‚úÖ good_time duplicate distribution: 1={duplicate_counts.count(1)}, "
                        f"2={duplicate_counts.count(2)}, 3={duplicate_counts.count(3)}, "
                        f"4={duplicate_counts.count(4)}, 5={duplicate_counts.count(5)}")
    
    def validate_dumb_time(self):
        """Validate dumb_time column."""
        print("\nüîç Validating dumb_time column...")
        
        # Identify groups (200-500 rows each)
        group_starts = [0]
        for i in range(200, len(self.df), 200):
            if i < len(self.df):
                # Look for empty dumb_time as group marker
                for j in range(max(0, i-100), min(len(self.df), i+100)):
                    if self.df['dumb_time'].iloc[j] == "":
                        if j not in group_starts:
                            group_starts.append(j)
                        break
        
        # Validate each group
        empty_first_row_issues = []
        time_comparison_issues = []
        
        for i in range(len(group_starts)):
            start = group_starts[i]
            end = group_starts[i+1] if i+1 < len(group_starts) else len(self.df)
            
            # Check first row has empty dumb_time
            if self.df['dumb_time'].iloc[start] != "":
                empty_first_row_issues.append((start, self.df['dumb_time'].iloc[start]))
            
            # Check other rows have valid dumb_time > good_time
            for row_idx in range(start + 1, end):
                dumb_val = self.df['dumb_time'].iloc[row_idx]
                good_val = self.df['good_time'].iloc[row_idx]
                
                if dumb_val == "":
                    time_comparison_issues.append((row_idx, "empty dumb_time in non-first row"))
                else:
                    try:
                        good_time = self.parse_time(good_val)
                        dumb_time = self.parse_time(dumb_val)
                        
                        time_diff = (dumb_time - good_time).total_seconds()
                        if time_diff < 60 or time_diff > 360:  # Should be 1-5 minutes
                            time_comparison_issues.append((row_idx, f"time diff = {time_diff}s (expected 60-360s)"))
                    except:
                        time_comparison_issues.append((row_idx, "invalid time format"))
        
        if empty_first_row_issues:
            self.errors.append(f"‚ùå Found {len(empty_first_row_issues)} groups where first row has non-empty dumb_time")
        else:
            self.info.append("‚úÖ All groups have empty dumb_time in first row")
        
        if time_comparison_issues:
            self.errors.append(f"‚ùå Found {len(time_comparison_issues)} dumb_time validation issues")
            for idx, issue in time_comparison_issues[:5]:
                self.errors.append(f"   Row {idx}: {issue}")
        else:
            self.info.append("‚úÖ All dumb_time values are 1-5 minutes after good_time")
    
    def validate_numerical_columns(self):
        """Validate numerical columns."""
        print("\nüîç Validating numerical columns...")
        
        # Width: 1.00-200.00, 2 decimal places
        width_issues = []
        for idx, val in enumerate(self.df['width']):
            try:
                width = float(val)
                if width < 1.00 or width > 200.00:
                    width_issues.append((idx, val, "out of range"))
                elif len(val.split('.')[-1]) != 2:
                    width_issues.append((idx, val, "not 2 decimal places"))
            except:
                width_issues.append((idx, val, "not a number"))
        
        if width_issues:
            self.errors.append(f"‚ùå Found {len(width_issues)} width validation issues")
        else:
            self.info.append("‚úÖ All width values valid (1.00-200.00, 2 decimals)")
        
        # Height: 0.2-4.8, 1 decimal place
        height_issues = []
        for idx, val in enumerate(self.df['height']):
            try:
                height = float(val)
                if height < 0.2 or height > 4.8:
                    height_issues.append((idx, val, "out of range"))
                elif len(val.split('.')[-1]) != 1:
                    height_issues.append((idx, val, "not 1 decimal place"))
            except:
                height_issues.append((idx, val, "not a number"))
        
        if height_issues:
            self.errors.append(f"‚ùå Found {len(height_issues)} height validation issues")
        else:
            self.info.append("‚úÖ All height values valid (0.2-4.8, 1 decimal)")
        
        # Angle: 0.00-360.00, 2 decimal places
        angle_issues = []
        for idx, val in enumerate(self.df['angle']):
            try:
                angle = float(val)
                if angle < 0.00 or angle > 360.00:
                    angle_issues.append((idx, val, "out of range"))
                elif len(val.split('.')[-1]) != 2:
                    angle_issues.append((idx, val, "not 2 decimal places"))
            except:
                angle_issues.append((idx, val, "not a number"))
        
        if angle_issues:
            self.errors.append(f"‚ùå Found {len(angle_issues)} angle validation issues")
        else:
            self.info.append("‚úÖ All angle values valid (0.00-360.00, 2 decimals)")
    
    def validate_categorical_columns(self):
        """Validate categorical columns."""
        print("\nüîç Validating categorical columns...")
        
        for i in range(3, 11):
            col_name = f'category_{i}'
            expected_values = [chr(ord('a') + j) for j in range(i)]
            unique_values = self.df[col_name].unique()
            
            invalid_values = [v for v in unique_values if v not in expected_values]
            if invalid_values:
                self.errors.append(f"‚ùå {col_name} has invalid values: {invalid_values}")
            else:
                self.info.append(f"‚úÖ {col_name} has correct values: {sorted(unique_values)}")
    
    def validate_boolean_columns(self):
        """Validate boolean columns."""
        print("\nüîç Validating boolean columns...")
        
        bool_columns = ['isGood', 'isOld', 'isWhat', 'isEnabled', 'isFlagged']
        for col in bool_columns:
            unique_values = self.df[col].unique()
            valid_values = ['True', 'False', 'TRUE', 'FALSE', 'true', 'false', '1', '0']
            
            invalid_values = [v for v in unique_values if v not in valid_values]
            if invalid_values:
                self.errors.append(f"‚ùå {col} has invalid boolean values: {invalid_values}")
            else:
                self.info.append(f"‚úÖ {col} has valid boolean values")
    
    def validate_inference_columns(self):
        """Validate inference stress test columns."""
        print("\nüîç Validating inference stress test columns...")
        
        inference_types = [
            'integer', 'real', 'text', 'boolean', 'date', 'datetime',
            'timeseconds', 'timemilliseconds', 'timemicroseconds', 'timenanoseconds', 'blob'
        ]
        
        for inf_type in inference_types:
            blank_col = f'{inf_type}_infer_blank'
            dash_col = f'{inf_type}_infer_dash'
            
            # Check blank column has empty values
            blank_values = self.df[blank_col].value_counts()
            if '' in blank_values:
                blank_count = blank_values['']
                self.info.append(f"‚úÖ {blank_col} has {blank_count} empty values ({blank_count/len(self.df)*100:.1f}%)")
            else:
                self.warnings.append(f"‚ö†Ô∏è  {blank_col} has no empty values")
            
            # Check dash column has "-" values
            dash_values = self.df[dash_col].value_counts()
            if '-' in dash_values:
                dash_count = dash_values['-']
                self.info.append(f"‚úÖ {dash_col} has {dash_count} dash values ({dash_count/len(self.df)*100:.1f}%)")
            else:
                self.warnings.append(f"‚ö†Ô∏è  {dash_col} has no dash values")
    
    def validate_tags_column(self):
        """Validate tags column."""
        print("\nüîç Validating tags column...")
        
        valid_tags = ["", "a", "a,b", "a,b,c"]
        unique_tags = self.df['tags'].unique()
        
        invalid_tags = [t for t in unique_tags if t not in valid_tags]
        if invalid_tags:
            self.errors.append(f"‚ùå Invalid tag values found: {invalid_tags}")
        else:
            self.info.append(f"‚úÖ All tag values are valid: {sorted(unique_tags)}")
    
    def validate_distribution_columns(self):
        """Validate distribution columns."""
        print("\nüîç Validating distribution columns...")
        
        dist_columns = ['bimodal', 'linear_over_time', 'exponential', 'uniform', 'normal']
        
        for col in dist_columns:
            try:
                values = pd.to_numeric(self.df[col], errors='coerce')
                if values.isna().any():
                    self.errors.append(f"‚ùå {col} contains non-numeric values")
                else:
                    self.info.append(f"‚úÖ {col}: mean={values.mean():.2f}, std={values.std():.2f}, "
                                   f"min={values.min():.2f}, max={values.max():.2f}")
                    
                    # Special checks
                    if col == 'normal' and abs(values.mean() - 50) > 5:
                        self.warnings.append(f"‚ö†Ô∏è  {col} mean is {values.mean():.2f} (expected ~50)")
                    
                    if col == 'bimodal':
                        # Check for two peaks around 30 and 70
                        hist, bins = np.histogram(values, bins=20)
                        # This is a simple check - could be more sophisticated
                        self.info.append(f"   Bimodal distribution histogram peaks detected")
                        
            except Exception as e:
                self.errors.append(f"‚ùå Error analyzing {col}: {e}")
    
    def validate_time_gaps(self):
        """Validate time gaps between groups, including hour-long gaps."""
        print("\nüîç Validating time gaps between groups...")
        
        # Find group boundaries (where dumb_time is empty)
        group_starts = [0]
        for i in range(1, len(self.df)):
            if self.df['dumb_time'].iloc[i] == "":
                group_starts.append(i)
        
        # Calculate gaps between groups
        gaps = []
        hour_gaps = 0
        
        for i in range(1, len(group_starts)):
            # Get last time of previous group
            prev_group_end = group_starts[i] - 1
            prev_time_str = self.df['good_time'].iloc[prev_group_end]
            
            # Get first time of current group
            curr_group_start = group_starts[i]
            curr_time_str = self.df['good_time'].iloc[curr_group_start]
            
            try:
                prev_time = self.parse_time(prev_time_str)
                curr_time = self.parse_time(curr_time_str)
                
                gap_seconds = (curr_time - prev_time).total_seconds()
                gaps.append(gap_seconds)
                
                # Check for hour gaps (3600 seconds ¬± 60 seconds for normal gap)
                if 3540 <= gap_seconds <= 3660:
                    hour_gaps += 1
            except:
                pass
        
        if gaps:
            normal_gaps = [g for g in gaps if g < 3540]
            if normal_gaps:
                avg_normal_gap = sum(normal_gaps) / len(normal_gaps)
                self.info.append(f"‚úÖ Average normal gap between groups: {avg_normal_gap:.1f} seconds")
            
            self.info.append(f"‚úÖ Found {hour_gaps} hour-long gaps between groups")
            
            # Check if ~10% of gaps are hour-long
            if len(gaps) > 0:
                hour_gap_percentage = (hour_gaps / len(gaps)) * 100
                self.info.append(f"‚úÖ Hour gaps: {hour_gap_percentage:.1f}% of all gaps")
                
                if abs(hour_gap_percentage - 10) > 5:  # Allow 5% tolerance
                    self.warnings.append(f"‚ö†Ô∏è  Hour gap percentage is {hour_gap_percentage:.1f}% (expected ~10%)")
        else:
            self.info.append("‚úÖ No gaps found (single group dataset)")
    
    def validate_group_duplication(self):
        """Validate group duplication pattern (80/15/5)."""
        print("\nüîç Validating group duplication pattern...")
        
        # This is a complex validation - we'll do a simplified check
        # by looking at patterns in non-time columns
        
        # Create a hash of non-time columns for each row
        non_time_cols = [col for col in self.df.columns if col not in ['good_time', 'dumb_time']]
        
        # Group by consecutive rows with same pattern
        groups = []
        i = 0
        while i < len(self.df):
            # Find group boundary (empty dumb_time)
            group_start = i
            while i < len(self.df) and (i == group_start or self.df['dumb_time'].iloc[i] != ""):
                i += 1
            
            if i > group_start:
                # Create signature for this group (excluding time columns)
                group_data = self.df.iloc[group_start:i][non_time_cols]
                groups.append({
                    'start': group_start,
                    'end': i,
                    'size': i - group_start,
                    'data': group_data
                })
        
        self.info.append(f"‚úÖ Found {len(groups)} groups, sizes: min={min(g['size'] for g in groups)}, "
                        f"max={max(g['size'] for g in groups)}")
        
        # Check for duplicate groups
        unique_groups = 0
        duplicated_once = 0
        duplicated_twice = 0
        
        checked = set()
        for i, group in enumerate(groups):
            if i in checked:
                continue
                
            # Count how many times this group pattern appears
            duplicates = 0
            for j in range(i + 1, len(groups)):
                if j in checked:
                    continue
                if group['size'] == groups[j]['size']:
                    # Compare data (excluding time columns)
                    if group['data'].equals(groups[j]['data']):
                        duplicates += 1
                        checked.add(j)
            
            if duplicates == 0:
                unique_groups += 1
            elif duplicates == 1:
                duplicated_once += 1
            elif duplicates == 2:
                duplicated_twice += 1
        
        total_groups = unique_groups + duplicated_once + duplicated_twice
        if total_groups > 0:
            unique_pct = unique_groups / total_groups * 100
            once_pct = duplicated_once / total_groups * 100
            twice_pct = duplicated_twice / total_groups * 100
            
            self.info.append(f"‚úÖ Group duplication: {unique_pct:.1f}% unique, "
                           f"{once_pct:.1f}% duplicated once, {twice_pct:.1f}% duplicated twice")
            
            if abs(unique_pct - 80) > 10:
                self.warnings.append(f"‚ö†Ô∏è  Unique group percentage is {unique_pct:.1f}% (expected ~80%)")
    
    def validate_column_count(self):
        """Validate total column count."""
        print("\nüîç Validating column count...")
        
        expected_columns = 46  # Based on requirements (2 time + 3 numeric + 8 categorical + 5 boolean + 22 inference + 1 tags + 5 distribution)
        actual_columns = len(self.df.columns)
        
        if actual_columns != expected_columns:
            self.warnings.append(f"‚ö†Ô∏è  Found {actual_columns} columns (expected {expected_columns})")
            
            # List all columns for debugging
            self.info.append("Column list:")
            for i, col in enumerate(self.df.columns):
                self.info.append(f"  {i+1}. {col}")
        else:
            self.info.append(f"‚úÖ Correct number of columns: {actual_columns}")
    
    def run_all_validations(self):
        """Run all validation checks."""
        print(f"\n{'='*60}")
        print(f"üîç Validating {self.filename}")
        print(f"{'='*60}")
        
        if not self.load_data():
            return False
        
        self.validate_column_count()
        self.validate_good_time()
        self.validate_dumb_time()
        self.validate_numerical_columns()
        self.validate_categorical_columns()
        self.validate_boolean_columns()
        self.validate_inference_columns()
        self.validate_tags_column()
        self.validate_distribution_columns()
        self.validate_time_gaps()
        self.validate_group_duplication()
        
        # Summary
        print(f"\n{'='*60}")
        print("üìä VALIDATION SUMMARY")
        print(f"{'='*60}")
        
        print(f"\n‚úÖ Info ({len(self.info)} items):")
        for info in self.info[:10]:  # Show first 10
            print(f"   {info}")
        if len(self.info) > 10:
            print(f"   ... and {len(self.info) - 10} more")
        
        if self.warnings:
            print(f"\n‚ö†Ô∏è  Warnings ({len(self.warnings)} items):")
            for warning in self.warnings:
                print(f"   {warning}")
        
        if self.errors:
            print(f"\n‚ùå Errors ({len(self.errors)} items):")
            for error in self.errors:
                print(f"   {error}")
            print("\n‚ùå VALIDATION FAILED")
            return False
        else:
            print("\n‚úÖ VALIDATION PASSED")
            return True

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Validate test data CSV file')
    parser.add_argument('--file', type=str, default='test_data.csv',
                       help='Path to CSV file to validate (default: test_data.csv)')
    args = parser.parse_args()
    
    validator = TestDataValidator(args.file)
    success = validator.run_all_validations()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()